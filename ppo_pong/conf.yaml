# Logger
experiment_name: atari_pong
agent_name: ppo_agent
entity: vmoens
wandb_key: null
log_dir: /tmp/atari_pong

# Environment
env_name: ALE/Pong-v5
frame_skip: 4

# Collector
total_frames: 40_000_000 # without accounting for frame skip
num_parallel_envs: 8
steps_per_env: 128 # between network updates

# Loss
gamma: 0.99
clip_epsilon: 0.1
loss_critic_type: l2
entropy_coef: 0.0001
critic_coef: 1.0
gae_lamdda: 0.95

# Training loop
lr: 2.5e-4
num_ppo_epochs: 3
mini_batch_size: 256  # so 4 mini_batches - (8 * 128) / 256
evaluation_frequency: 500  # In number of network updates
